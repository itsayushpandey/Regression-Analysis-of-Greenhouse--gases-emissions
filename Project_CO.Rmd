---
title: "STATSII_project"
author: "Anup Bhutada, Ayush Pandey, Shouvik Sengupta, Uday Gadge"
date: "2023-05-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Predicting Green house emissions from Sensor data

Following is a data set collected from experiment set up to measure green house emissions from a turbine engine. There are different sensors set up to measure certain dependent features like temperature, pressure etc. The goal is to see to what extent these measurements can be useful to predict the emission of green house gases. There are two features here that measure these emissions `NOx` and `CO`.

**Reading the data**

The data is formatted year wise, so it needs to be merged.

```{r}
library(tidyr)
library(tidyverse)
library(ggplot2)

path <- 'C:/Users/gadge/OneDrive/Desktop/STATS-II/Project/'
df1 <- read_csv(paste(path, 'gt_2011.csv', sep = '')) %>% mutate(year = 2011)
df2 <- read_csv(paste(path, 'gt_2012.csv', sep = '')) %>% mutate(year = 2012)
df3 <- read_csv(paste(path, 'gt_2013.csv', sep = '')) %>% mutate(year = 2013)
df4 <- read_csv(paste(path, 'gt_2014.csv', sep = '')) %>% mutate(year = 2014)
df5 <- read_csv(paste(path, 'gt_2015.csv', sep = '')) %>% mutate(year = 2014)

df <- rbind(df1, df2, df3, df4, df5)
head(df)

```
```{r}
(colnames(df))
(dim(df))
```
** Data Exploration and Visualization **

The data has only numerical features.

```{r}
library(reshape2)
p <- ggplot(melt(df), aes(factor(variable), value)) 
p + geom_boxplot(fill = "#CFB87C") + facet_wrap(~variable, scale="free")+xlab("")+ylab("")+
  theme_minimal()
```

```{r}

library(reshape2)
p <- ggplot(melt(df1), aes(x=value)) 
p + geom_histogram(bins = 50, fill = "#CFB87C",col = "black") + facet_wrap(~variable, scale="free") + xlab("")+ylab("")+
  theme_minimal()

```
Exploring correlations between features

```{r}
pairs(df[,-12],col = "slategrey", main = "Pair plots")
```
```{r}
library(corrplot)
col4 = colorRampPalette(c("black", "darkgrey", "grey","#CFB87C"))
corrplot(cor(df[,-12]), method = "ellipse", col = col4(100),  addCoef.col = "black", tl.col = "black")
```
There are some features with high correlation, looking at them closely.

```{r}
pairs(df[c('AFDP', 'GTEP', 'TIT', 'TEY', 'CDP', 'CO')], col = "darkgrey",main = "Pair plots")
```
```{r}
library(corrplot)
col4 = colorRampPalette(c("black", "darkgrey", "grey","#CFB87C"))
corrplot(cor(df[c('AFDP', 'GTEP', 'TIT', 'TEY', 'CDP' , 'CO')]),
         method = "ellipse", col = col4(100),  addCoef.col = "black", tl.col = "black")
```
```{r}
lmod <- lm(CO ~ . - NOX - year - CDP, data = df)
summary(lmod)
```
```{r}
par(mfrow = c(2,2))
plot(lmod,col = "darkgrey")
```
```{r}
ggplot(mapping = aes(fitted(lmod),df$CO)) + geom_point(col = "darkgrey")+
  geom_abline(slope = 1, intercept = 0, col = 'black',linetype = 'dashed')+
  geom_smooth(se = FALSE, col = "#CFB87C")+theme_bw()+xlab("Fitted") + ylab("Actual")
```
From the above plots, some key violations of linear regression are:
- Linearity
- Normality 

They both look interlinked as from the histogram of `CO` it didn't look normal. The fitted vs actual shows a parabolic curve. 

The relation now looks like
$$ y = (\beta_0 + \beta_1x_1+...\beta_px_p)^2$$
A square root transformation would make this a linear. It might make it normal as well
$$ \sqrt y = \beta_0 + \beta_1x_1+...\beta_px_p $$

```{r}
par(mfrow = c(1,2))
hist(df$CO,col = "#CFB87C",main = "No Transformation",xlab = "",ylab = "")
hist(sqrt(df$CO),col = "#CFB87C",main = "Square Root Transformation",xlab = "",ylab = "")

```

** Transformation **

The transformation helped with normality to an extent as well. Fitting the model with transformation.
```{r}
lm_transform = lm(sqrt(CO) ~ . - year - NOX - CDP, data = df)
summary(lm_transform)
```
```{r}
par(mfrow = c(2,2))
plot(lm_transform, col = "darkgrey")
```
```{r}
ggplot(mapping = aes(fitted(lm_transform),sqrt(df$CO))) + geom_point(col = "darkgrey")+
  geom_abline(slope = 1, intercept = 0, col = 'black',linetype = 'dashed')+
  geom_smooth(se = FALSE, col = "#CFB87C")+theme_bw()+xlab("Fitted") + ylab("Actual")
```
Linearity is fixed!!

Nomrality is still an issue. 

Homoscedasticity looks violated to an extent. A weighted Least squares can be done to check for this.

The only measure that can be used to compare the pre transformed model and post transformed model is $R^2$ and $R_a^2$. `AIC` and `BIC` use `RSS` which changes as the response is scaled differently. The $R^2$ improved quite significantly with the transformation.

** Weighted Least Squares **

Looking at `CO` against all other features to decide the feature that can give an estimate on variance in `CO`

```{r}
df.gathered <- df[,-12] %>% gather(key = "variable", value = "value",-CO)
df.gathered %>% ggplot(aes(y = CO,x = value)) + geom_point(col = 'darkgrey')+
  facet_wrap(~variable,scales = 'free')+theme_minimal()
```

```{r}
i = order(df$TEY)
ndf = df[i,]

ff = head(gl(1837,20), 36733)

meanTEY = unlist(lapply(split(ndf$TEY,ff),mean))

varCO = unlist(lapply(split(sqrt(ndf$CO),ff),var)); 

ggplot(mapping = aes(x = meanTEY, y = varCO)) + geom_point(col = 'darkgrey') +
  geom_smooth(se = FALSE, col = "#CFB87C") + xlab("Mean of TEY") + ylab("Variance in sqrt of CO")

```
```{r}
ggplot(mapping = aes(x = meanTEY, y = log(varCO))) + geom_point(col = 'darkgrey') +
  geom_smooth(se = FALSE, col = "#CFB87C") + xlab("Mean of TEY") + ylab("Log of variance in sqrt of CO")

```
```{r}
lm_var = lm(log(varCO) ~ meanTEY)
summary(lm_var)
```
```{r}
par(mfrow = c(2,2))
plot(lm_var, col = 'darkgrey')
```
It's a fairly decent model to use to estimate variance.

```{r}
var_resp <- exp(predict(lm_var, data.frame(meanTEY = df$TEY)))
length(var_resp)

weights <- 1/var_resp
length(weights)
```
```{r}
lmodwls <- lm(sqrt(CO) ~ . - year - NOX - CDP, data = df, weights = weights)
summary(lmodwls)
```

```{r}
par(mfrow = c(2,2))
plot(lmodwls, col = "darkgrey")
```
```{r}
ggplot(mapping = aes(fitted(lmodwls),sqrt(df$CO))) + geom_point(col = "darkgrey")+
  geom_abline(slope = 1, intercept = 0, col = 'black',linetype = 'dashed')+
  geom_smooth(se = FALSE, col = "#CFB87C")+theme_bw()+xlab("Fitted") + ylab("Actual")

```


```{r}
summary(weights)
```
The reason `WLS` didn't work is the variance we tried estimated variance using `TEY` and that model has a very low $R^2$ indicating that the weights are not calculated properly.



** Multicolinearity check **
```{r}
library(car)
vif(lm_transform)
```
There are features with high VIF.

```{r}
kappa(lm_transform)
```
The value of kappa is also too high. This is not a least variance model.


## Model selection

We can try to remove the predictor with highest VIF value, but this caused other predictors to become insignificant. Therefore we are using a systematic approach to remove multicollinearity.

```{r}
library(leaps)
library(MASS)

n = dim(df)[1]; 
reg1 = regsubsets(sqrt(CO) ~ . - year - NOX - CDP, data = df)
rs = summary(reg1)
rs$which
```


```{r}
AIC <- 2*(2:9) + n*log(rs$rss/n)
plot(AIC ~ I(1:8), xlab = "number of predictors", ylab = 'AIC')
```

```{r}
plot(1:8, rs$adjr2, xlab = "number of predictors", ylab = "adj R squared")
```
```{r}
BIC = log(n)*(2:9) + n*log(rs$rss/n) 
plot(BIC ~ I(1:8), xlab = "number of predictors", ylab = "BIC")
```
```{r}
lm_6 <- lm(sqrt(CO) ~ . - year - NOX - CDP - AP - AFDP, data = df)
summary(lm_6)
```
```{r}
vif(lm_6)
```
```{r}
kappa(lm_6)
```
Removal of further features would decrease the metrics, so regularization to see if we can get a model with lower variance.

** Regularization**

```{r}
df_norm <- df %>% mutate(CO_sqrt = sqrt(CO))
df_norm <- df_norm %>% dplyr::select(-c(CO))

df_norm <-  as.data.frame(scale(df_norm))

lm_norm <- lm(CO_sqrt ~ . - year - NOX - CDP, data = df_norm)
summary(lm_norm)
```
```{r}
lm_ridge <- lm.ridge(CO_sqrt ~ . - year - NOX - CDP - 1, data = df_norm, lambda = seq(0, 10, 0.01))
lm_ridge_df <- as.data.frame(t(lm_ridge$coef))
lm_ridge_df$lambda <- lm_ridge$lambda
lm_ridge_df <- lm_ridge_df%>% gather(key = "variable", value = "value", -lambda)
lm_ridge_df %>% ggplot(aes(lambda, value)) + geom_point() + geom_smooth(se = FALSE, col = "#CFB87C")+
  xlab('lambda') + ylab('Coefficient') + facet_wrap(~variable) + theme_minimal()

```
No smoothing is happening
```{r}
(mod <- select(lm_ridge))
```
```{r}
lm_ridge_l <- lm.ridge(CO_sqrt ~ . - year - NOX - CDP - 1, data = df_norm, lambda = 3.8)
```
```{r}
y.pred <- as.matrix(df_norm %>% dplyr::select(-c(CDP, NOX, year, CO_sqrt))) %*% coef(lm_ridge_l)
length(y.pred)
```
```{r}
ggplot(mapping = aes(x = y.pred, y = df_norm$CO_sqrt)) + geom_point(col = "darkgrey")+
  geom_abline(slope = 1, intercept = 0, col = 'black',linetype = 'dashed')+
  geom_smooth(se = FALSE, col = "#CFB87C")+theme_bw()+xlab("Fitted") + ylab("Actual")
```
The regularization isn't affecting anything. We can test MSPE of this with the original model to see if it's better.

** Correlated Errors**
```{r}
library(gridExtra)
ind <- order(df$CO)
p1 <- ggplot(mapping = aes(x = 1:length(df$CO), y = residuals(lm_6)[ind])) +
  geom_point(alpha = 0.5, col = 'darkgrey') +
  geom_abline(slope = 0, intercept = 0) +
  xlab("Index") +
  geom_smooth(se = F, col = "#CFB87C") +
  ylab("Residuals") +
  theme_bw()+ggtitle("Erros against ordered CO")

ordered.reiduals <- residuals(lm_6)[ind]
p2 <- ggplot(mapping = aes(x = ordered.reiduals[-length(ordered.reiduals)],
                           y = ordered.reiduals[-1]))+
               geom_point(alpha = 0.5, col = 'darkgrey') +
               geom_abline(slope = 0, intercept = 0) +
               xlab("E[i]") +
               geom_smooth(se = F, col = "#CFB87C") +
  ylab("E[i-1]") +
  theme_bw()+ggtitle("Consecutive Errors")

grid.arrange(p1,p2, nrow = 1)
```

```{r}
(cor.error <- cor(ordered.reiduals[-1],ordered.reiduals[-length(ordered.reiduals)]))
```

There is some correlation in error terms.

** GLS **


```{r}
library(nlme)
df_ordered <- df[ind,]
df_ordered$index <- 1:length(ind)
gls.6 <- gls(sqrt(CO) ~ . - year - NOX - CDP - AP - AFDP - index, data = df_ordered, correlation = corAR1(form = ~ 1 | index))
summary(gls.6)

```


```{r}
plot(gls.6, col = "darkgrey")
```
```{r}
ggplot(mapping = aes(x = fitted(gls.6), y = sqrt(df_ordered$CO))) + geom_point(col = "darkgrey")+
  geom_abline(slope = 1, intercept = 0, col = 'black',linetype = 'dashed')+
  geom_smooth(se = FALSE, col = "#CFB87C")+theme_bw()+xlab("Fitted") + ylab("Actual")
```

```{r}
ind <- order(df$CO)
p1 <- ggplot(mapping = aes(x = 1:length(df$CO), y = residuals(gls.6)[ind])) +
  geom_point(alpha = 0.5, col = 'darkgrey') +
  geom_abline(slope = 0, intercept = 0) +
  xlab("Index") +
  geom_smooth(se = F, col = "#CFB87C") +
  ylab("Residuals") +
  theme_bw()+ggtitle("Errors with index")

ordered.reiduals <- residuals(gls.6)[ind]
p2 <- ggplot(mapping = aes(x = ordered.reiduals[-length(ordered.reiduals)],
                           y = ordered.reiduals[-1]))+
               geom_point(alpha = 0.5, col = 'darkgrey') +
               geom_abline(slope = 0, intercept = 0) +
               xlab("E[i]") +
               geom_smooth(se = F, col = "#CFB87C") +
  ylab("E[i-1]") +
  theme_bw()+ggtitle("Consecutive Errors")

grid.arrange(p1,p2, nrow = 1)
```

```{r}
(cor.error <- cor(ordered.reiduals[-1],ordered.reiduals[-length(ordered.reiduals)]))
```
** Checking MSPEs and MAE **

For all the models try to get MSPE and MAE by splitting it into train and test split

```{r}
set.seed(12)
n = floor(0.8 * nrow(df1))
index = sample(seq_len(nrow(df1)), size = n)

train = df1[index, ]
test = df1[-index, ]
head(train)
summary(train)
dim(train)
```
Model with the 6 predictors

```{r}

lm_mspe_6 <- lm(sqrt(CO) ~ AT + AH + GTEP + TIT + TAT + TEY, data = train)
summary(lm_mspe_6)

```
```{r}
sqrt_CO <- predict(lm_mspe_6, newdata = test[, c('AT', 'AH', 'GTEP', 'TIT', 'TAT', 'TEY')])
pred_CO <- sqrt_CO^2
length(pred_CO)
```

```{r}
mspe <- mean((pred_CO - test$CO)^2)
mspe
mae <- mean(abs(pred_CO - test$CO))
mae
```
Ridge Regression

```{r}
train_norm <- train %>% mutate(CO_sqrt = sqrt(CO))
train_norm <- train_norm %>% dplyr::select(-c(CO))

train_sd <- unlist(lapply(train_norm, sd))
#train_sd
train_mean <- unlist(lapply(train_norm, mean))
#train_mean

train_norm <-  as.data.frame(scale(train_norm))
#train_norm


test_norm <- test %>% mutate(CO_sqrt = sqrt(CO))
test_norm <- test_norm %>% dplyr::select(-c(CO))

test_norm <- sweep(sweep(as.matrix(test_norm), 2, t(as.matrix(train_mean))), 2, train_sd, FUN = '/')
test_norm <- as.data.frame(test_norm)
unlist(lapply(test_norm, mean))
unlist(lapply(test_norm, sd))


```

```{r}
lm_ridge_l <- lm.ridge(CO_sqrt ~ AT + AP + AH + AFDP + GTEP + TIT + TAT + TEY + 0, data = train_norm, lambda = 3.8)
```
```{r}
y.pred <- as.matrix(test_norm %>% dplyr::select(c("AT","AP","AH","AFDP","GTEP","TIT","TAT","TEY"))) %*% coef(lm_ridge_l)
length(y.pred)
```

```{r}
y.pred_unnorm <- (y.pred*train_sd['CO_sqrt'] + train_mean['CO_sqrt'])^2
length(y.pred_unnorm)

mspe <- mean((test$CO - y.pred_unnorm)^2)
mspe

mae <- mean(abs(test$CO - y.pred_unnorm))
mae
```
** GLS **

```{r}
library(nlme)
ind <- order(train$CO)
train_ordered <- train[ind,]
train_ordered$index <- 1:length(ind)
gls.6 <- gls(sqrt(CO) ~ AT + AH + GTEP + TIT + TAT + TEY - index, data = train_ordered, correlation = corAR1(form = ~ 1 | index))
summary(gls.6)

```


```{r}
y.pred <- as.matrix(cbind(1,test %>% dplyr::select(c("AT","AH","GTEP","TIT","TAT","TEY")))) %*% gls.6$coefficients
y.pred <- y.pred^2
```

```{r}
mspe <- mean((test$CO - y.pred)^2)
mspe

mae <- mean(abs(test$CO - y.pred))
mae

```


